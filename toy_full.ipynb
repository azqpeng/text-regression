{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from TextDataset import TextDataset\n",
    "from gnn_model import gcn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from collections import OrderedDict\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab is complete\n",
      "Vocab is complete\n",
      "computations complete\n",
      "PMI is complete\n",
      "TFIDF is complete\n",
      "Vocab is complete\n",
      "This is the: 29\n",
      "computations complete\n",
      "PMI is complete\n",
      "TFIDF is complete\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 147\u001b[0m\n\u001b[0;32m    144\u001b[0m adj \u001b[39m=\u001b[39m build_graph(docList, \u001b[39m\"\u001b[39m\u001b[39mPMI\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[0;32m    146\u001b[0m \u001b[39m# create dataset\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m data \u001b[39m=\u001b[39m TextDataset(docList, labels, \u001b[39mlen\u001b[39;49m(docList) \u001b[39m+\u001b[39;49m \u001b[39mlen\u001b[39;49m(vocab), test_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\azqpe\\periodlearning\\TextDataset.py:19\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[1;34m(self, docList, labels, nodeCount, test_size, window, transform)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meye(nodeCount)\n\u001b[0;32m     18\u001b[0m \u001b[39m# create adjacency matrix\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madj \u001b[39m=\u001b[39m build_graph(docList, \u001b[39m\"\u001b[39;49m\u001b[39mPMI\u001b[39;49m\u001b[39m\"\u001b[39;49m, window \u001b[39m=\u001b[39;49m window)\n\u001b[0;32m     21\u001b[0m \u001b[39m# split data into training, validation(?), test\u001b[39;00m\n\u001b[0;32m     22\u001b[0m train_data, test_data, train_labels, test_labels \u001b[39m=\u001b[39m train_test_split(pd\u001b[39m.\u001b[39mSeries(docList), pd\u001b[39m.\u001b[39mSeries(labels), test_size \u001b[39m=\u001b[39m test_size)\n",
      "File \u001b[1;32mc:\\Users\\azqpe\\periodlearning\\build_graph.py:59\u001b[0m, in \u001b[0;36mbuild_graph\u001b[1;34m(docList, wordEdges, window)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTFIDF is complete\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[39m# return adjacency matrix using A^ = D^-1/2 * A * D^-1/2\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m diag \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdiag(np\u001b[39m.\u001b[39;49msum(adj, axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m))\u001b[39m^\u001b[39;49m(\u001b[39m-\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[0;32m     60\u001b[0m adj \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(np\u001b[39m.\u001b[39mmatmul(diag, adj), diag)\n\u001b[0;32m     61\u001b[0m \u001b[39m# adj = sparse.csr_matrix(adj)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# create docList\n",
    "docList = []\n",
    "with open(\"data/gutenold/toy_sentences.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        docList.append(line.split('\\t')[1].strip())\n",
    "f.close()\n",
    "\n",
    "# create labels\n",
    "labels = []\n",
    "with open(\"data/gutenold/toy_labels.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        labels.append(line.split('\\t')[1].strip())\n",
    "f.close()\n",
    "\n",
    "# create vocab\n",
    "def get_vocab(docList):\n",
    "    \"\"\"\n",
    "    This function takes the dataset and generates the list of vocab with their number of appearances as well using CountVectorizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # docList = [] # get_vocab(datapath + \"_docs.txt\")\n",
    "    # docNames = []\n",
    "    # f = open(docPath, 'rb')\n",
    "    # for line in f.readlines():\n",
    "    #     docList.append(line.decode('UTF-8').split('\\t')[1].strip())\n",
    "    #     docNames.append(line.decode('UTF-8').split('\\t')[0].strip())\n",
    "    # f.close()\n",
    "\n",
    "\n",
    "    tfidf = TfidfVectorizer(min_df=3)\n",
    "    tfidfvect = tfidf.fit_transform(docList)\n",
    "    vocab = tfidf.get_feature_names_out()\n",
    "    print(\"Vocab is complete\")\n",
    "\n",
    "    return vocab, tfidfvect\n",
    "\n",
    "\n",
    "vocab, tfidfvect = get_vocab(docList)\n",
    "def PMIEdges(docList, wordList, window, adj):\n",
    "\n",
    "    wordList = tuple(wordList)\n",
    "    wordSet = set(wordList)\n",
    "\n",
    "    # initializations\n",
    "    n_i  = OrderedDict((name, 0) for name in wordList)\n",
    "    word2index = OrderedDict((name,index) for index,name in enumerate(wordList))\n",
    "    occurrences = np.zeros( (len(wordList),len(wordList)) ,dtype=np.int32)\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    # count word occurences and co-occurences\n",
    "    for l in docList:\n",
    "        docSplit = l.split()\n",
    "        for i in range(len(docSplit) - window + 1):\n",
    "            # total windows #W\n",
    "            counter +=1\n",
    "            d = docSplit[i:i+window]\n",
    "            e = set()\n",
    "            # occurences of words #W(i))\n",
    "            for word in d:\n",
    "                if word in wordSet:\n",
    "                    n_i[word] += 1\n",
    "                    e.add(word)\n",
    "            # co-occurences of words #W(i, j)\n",
    "            for w1,w2 in combinations(e,2):\n",
    "                i1 = word2index[w1]\n",
    "                i2 = word2index[w2]\n",
    "                occurrences[i1][i2] += 1\n",
    "                occurrences[i2][i1] += 1\n",
    "\n",
    "    \n",
    "    pmi = occurrences/counter\n",
    "\n",
    "    for word in n_i:\n",
    "        if n_i[word] == 0:\n",
    "            print(word)\n",
    "\n",
    "    # perform the computations\n",
    "    p_i = np.array(list(n_i.values()))/counter\n",
    "    for col in range(len(wordList)):\n",
    "        pmi[:, col] = pmi[:, col]/p_i[col]\n",
    "    for row in range(len(wordList)):\n",
    "        pmi[row, :] = pmi[row,:]/p_i[row]\n",
    "    pmi = pmi + 1e-9\n",
    "    for col in range(len(wordList)):\n",
    "        pmi[:, col] = np.log(pmi[:, col])\n",
    "\n",
    "    print(\"computations complete\")\n",
    "    \n",
    "\n",
    "    # add into adjacency matrix\n",
    "    for i in range(len(wordList)):\n",
    "        for j in range(len(wordList)):\n",
    "            if i == j:\n",
    "                adj[i,j] = 1\n",
    "            elif pmi[i, j] > 0:\n",
    "                adj[i,j] = pmi[i,j]\n",
    "\n",
    "    print(\"PMI is complete\")\n",
    "    return adj\n",
    "\n",
    "\n",
    "def build_graph(docList, wordEdges = \"PMI\", window = 10):\n",
    "    \"\"\"\n",
    "    This function takes the dataset and generates an adjacency matrix based on the specifications in Yao et al. (2019).\n",
    "    Input: string representing path to the dataset, not including entire filename and only it's prefix (e.g. \"guten\")\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize variables\n",
    "    numDocs = len(docList)\n",
    "    wordList, tfvect = get_vocab(docList)\n",
    "    numWords = len(wordList)\n",
    "    numNodes =  numWords + numDocs\n",
    "\n",
    "    # build empty adjacency matrix (sparse) # note here that the first numWords indices are words, and the last numDocs indices are documents\n",
    "    adj = np.identity(numNodes)\n",
    "\n",
    "    # build word-to-doc edges\n",
    "    if wordEdges == \"PMI\":\n",
    "        adj = PMIEdges(docList, wordList, window, adj)\n",
    "\n",
    "    # build word-to-doc edges using TF-IDF\n",
    "    tfiter = tfvect.toarray()\n",
    "    for words in range(tfiter.shape[1]):\n",
    "        for docs in range(tfiter.shape[0]):\n",
    "            if tfiter[docs, words] > 0:\n",
    "                adj[words, docs + numWords] = tfiter[docs, words]\n",
    "                adj[docs+numWords, words] = tfiter[docs, words]\n",
    "    print(\"TFIDF is complete\")\n",
    "\n",
    "    # return adjacency matrix using A^ = D^-1/2 * A * D^-1/2\n",
    "    diag = np.diag(np.power(np.sum(adj, axis = 1), -0.5))\n",
    "    adj = np.matmul(np.matmul(diag, adj), diag)\n",
    "\n",
    "    # adj = sparse.csr_matrix(adj)\n",
    "\n",
    "    return adj\n",
    "\n",
    "\n",
    "\n",
    "# build adj\n",
    "adj = build_graph(docList, \"PMI\", 5)\n",
    "\n",
    "# create dataset\n",
    "data = TextDataset(docList, labels, len(docList) + len(vocab), test_size = 1/3)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.eye(len(docList) + len(vocab))\n",
    "model = gcn(X, 16, adj)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train_model(epochs = 100):\n",
    "    for epoch in range(1, epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        F.mse_loss(model()[data.train_mask], data.y_train[data.train_mask]).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model():\n",
    "    model.eval()\n",
    "    logits = model()\n",
    "    mask1 = data.train_mask\n",
    "    pred1 = logits[mask1].max(1)[1]\n",
    "    return \n",
    "\n",
    "train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "periodlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
